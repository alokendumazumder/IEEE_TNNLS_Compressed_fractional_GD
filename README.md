# Fractional Gradient Descent with Matrix Stepsizes (CFGD)

### Overview
This repository contains the official implementation of the algorithms proposed in the paper:
> **Fractional Gradient Descent with Matrix Stepsizes for Non-Convex Optimisation**  
> *Authors: Alokendu Mazumder, Keshav Vyas, and Punit Rathore*  
> *Robert Bosch Center for Cyber Physical Systems, Indian Institute of Science, Bengaluru*  
> *Preprint posted on TechRxiv, 2025*

---

## ðŸš€ Introduction
This work introduces two algorithms:
- **CFGD-1**
- **CFGD-2** (also referred to as **Distributed CFGD (DCFGD)** in the distributed setting)

Both are novel extensions of fractional gradient descent (FGD), designed for non-convex and matrix-smooth optimisation problems. CFGD incorporates **matrix-valued stepsizes** and **compression mechanisms**, allowing efficient large-scale distributed training.

The algorithms extend standard and fractional gradient descent to the distributed and federated learning domains, showing improved convergence and communication efficiency.

---

## ðŸ“‚ Repository Structure
```
â”œâ”€â”€ cfgd_vs_cgd.py       # Implementation of CFGD and DC(FGD) algorithms
â”œâ”€â”€ plot.py              # Visualization utilities for convergence and comparison
â”œâ”€â”€ experiments.py       # Experimental setup for single-node and distributed cases
â”œâ”€â”€ get_data.py          # Dataset loading and preprocessing
â”œâ”€â”€ get_scheduler.py     # Learning rate scheduler utilities
â”œâ”€â”€ models.py            # Model definitions for experiments
â”œâ”€â”€ utils.py             # Helper functions
â”œâ”€â”€ figures/             # Folder containing all result figures (9 plots assumed)
â”‚   â”œâ”€â”€ fig1.png
â”‚   â”œâ”€â”€ fig2.png
â”‚   â”œâ”€â”€ fig3.png
â”‚   â”œâ”€â”€ fig4.png
â”‚   â”œâ”€â”€ fig5.png
â”‚   â”œâ”€â”€ fig6.png
â”‚   â”œâ”€â”€ fig7.png
â”‚   â”œâ”€â”€ fig8.png
â”‚   â””â”€â”€ fig9.png
â””â”€â”€ README.md            # Project documentation (this file)
```

---

## âš™ï¸ Algorithms
The repository implements the following key algorithms:

- **CFGD-1:** Compressed Fractional Gradient Descent with matrix stepsize D applied before sketching.
- **CFGD-2:** Variant where sketching precedes the matrix stepsize operation.
- **DCFGD-1 and DCFGD-2:** Distributed versions of CFGD-1 and CFGD-2 for federated environments.

These are designed to handle both **single-node** and **multi-client distributed setups** efficiently.

---

## ðŸ§  Key Ideas
- Introduces **matrix-valued stepsizes** to leverage structure in non-convex matrix-smooth objectives.
- Employs **fractional-order gradients (Caputo derivative)** to accelerate convergence.
- Incorporates **communication-efficient sketching/compression** to reduce distributed overhead.
- Demonstrates theoretical **O(1/âˆšT)** convergence for matrix-smooth non-convex functions.
- Provides practical improvements in both iteration and communication complexity compared to DCGD and det-CGD.

---

## ðŸ§© Implementation Highlights
- **cfgd_vs_cgd.py:** Core implementation of CFGD-1, CFGD-2, DCFGD-1, and DCFGD-2.
- **plot.py:** Includes plotting utilities to reproduce convergence plots.
- **experiments.py:** Recreates results for logistic regression tasks in both single-node and distributed settings.
- **utils.py:** Provides general helper functions and reproducibility tools.

---

## ðŸ§ª Experiments
The experiments are divided into two categories:

### 1. Single Node Experiments
- Tests convergence of CFGD-1 and CFGD-2 on logistic regression tasks.
- Compares against vanilla GD, FGD, and DCGD.
- Demonstrates faster convergence when using matrix-valued stepsizes.

### 2. Distributed Experiments
- Evaluates DCFGD-1 and DCFGD-2 in federated setups.
- Compares performance with DCGD, det-CGD, and det-MARINA.
- Shows superior communication and iteration efficiency.

---

## ðŸ“ˆ Results
Below is a placeholder 3Ã—3 results grid showing sample figures from the `figures/` folder.

| ![](figures/fig1.png) | ![](figures/fig2.png) | ![](figures/fig3.png) |
|:----------------------:|:----------------------:|:----------------------:|
| ![](figures/fig4.png) | ![](figures/fig5.png) | ![](figures/fig6.png) |
| ![](figures/fig7.png) | ![](figures/fig8.png) | ![](figures/fig9.png) |

> **Figure 1â€“9:** Comparative performance of CFGD and DCFGD under different sketches and step-size configurations. *(You can update these captions later.)*

---

## ðŸ“š Citation
If you use this repository or build upon this work, please cite:

```bibtex
@article{mazumder2025cfgd,
  title={Fractional Gradient Descent with Matrix Stepsizes for Non-Convex Optimisation},
  author={Alokendu Mazumder and Keshav Vyas and Punit Rathore},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2025}
}
```

---

## ðŸ§¾ License
This repository is released under the **MIT License**. Please see the `LICENSE` file for more details.

---

## ðŸ™Œ Acknowledgements
This research was conducted at the **Robert Bosch Center for Cyber-Physical Systems**, Indian Institute of Science (IISc), Bengaluru.  
We thank the open-source community for providing supporting packages such as PyTorch, NumPy, and CVXPY.

---

## ðŸ“¬ Contact
For questions or collaborations, please contact:
- **Alokendu Mazumder** â€” alokendum@iisc.ac.in
- **Punit Rathore** â€” prathore@iisc.ac.in

---

> *This code accompanies the paper "Fractional Gradient Descent with Matrix Stepsizes for Non-Convex Optimisation" (TechRxiv Preprint, 2025).*

